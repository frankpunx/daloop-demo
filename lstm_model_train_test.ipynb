{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1655c8c",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54d206e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('UrbanEV-main/code')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90feab18",
   "metadata": {},
   "source": [
    "## 2. Define LSTM Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c24e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lstm(nn.Module):\n",
    "    def __init__(self, seq, n_fea, node=275):\n",
    "        super(Lstm, self).__init__()\n",
    "        self.num_feat = n_fea\n",
    "        self.nodes = node\n",
    "        self.seq_len = seq\n",
    "        self.lstm_hidden_dim = 16\n",
    "        self.lstm = nn.LSTM(input_size=n_fea, hidden_size=self.lstm_hidden_dim, num_layers=2,\n",
    "                            batch_first=True)\n",
    "        self.linear = nn.Linear(seq * self.lstm_hidden_dim, 1)\n",
    "\n",
    "    def forward(self, occ, extra_feat=None):  # occ.shape = [batch, node, seq]\n",
    "        x = occ.unsqueeze(-1)\n",
    "        if extra_feat is not None and extra_feat != 'None':\n",
    "            x = torch.cat([occ.unsqueeze(-1), extra_feat], dim=-1)\n",
    "        assert x.shape[-1] == self.num_feat, f\"Number of features ({x.shape[-1]}) does not match n_fea ({self.num_feat}).\"\n",
    "\n",
    "        bs = x.shape[0]\n",
    "        x = x.view(bs * self.nodes, self.seq_len, self.num_feat)\n",
    "\n",
    "        lstm_out, _ = self.lstm(x)  # lstm_out shape: [batch_size * node, seq_len, lstm_hidden_dim]\n",
    "        lstm_out = lstm_out.reshape(bs, self.nodes, self.seq_len * self.lstm_hidden_dim)\n",
    "        x = self.linear(lstm_out)\n",
    "        x = torch.squeeze(x)\n",
    "        return x\n",
    "\n",
    "print(\"LSTM model defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d790df",
   "metadata": {},
   "source": [
    "## 3. Load Data and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab588eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.feat = 'occ'  # Feature type: occupancy\n",
    "        self.pred_len = 3  # Prediction horizon (3 hours ahead)\n",
    "        self.seq_len = 12  # Look-back window (12 hours)\n",
    "        self.add_feat = 'None'  # No additional features\n",
    "        self.pred_type = 'region'  # Predict all regions/stations\n",
    "        self.fold = 6  # Cross-validation fold\n",
    "        self.total_fold = 6  # Total number of folds (months)\n",
    "        self.seed = 42\n",
    "        self.bs = 32  # Batch size\n",
    "        self.epoch = 50  # Training epochs\n",
    "\n",
    "args = Args()\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# Train-validation-test split ratios\n",
    "TRAIN_RATIO, VAL_RATIO, TEST_RATIO = 0.8, 0.1, 0.1\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Feature: {args.feat}\")\n",
    "print(f\"  Prediction horizon: {args.pred_len} hour(s)\")\n",
    "print(f\"  Look-back window: {args.seq_len} hours\")\n",
    "print(f\"  Prediction type: {args.pred_type}\")\n",
    "print(f\"  Batch size: {args.bs}\")\n",
    "print(f\"  Training epochs: {args.epoch}\")\n",
    "print(f\"  Train/Val/Test ratio: {TRAIN_RATIO}/{VAL_RATIO}/{TEST_RATIO}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0495ad1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load station information\n",
    "inf = pd.read_csv('UrbanEV-main/data/inf.csv', header=0, index_col=None)\n",
    "print(f\"Total stations: {len(inf)}\")\n",
    "print(f\"\\nStation information:\")\n",
    "print(inf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9a962f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load occupancy data\n",
    "occ_df = pd.read_csv('UrbanEV-main/data/occupancy.csv', header=0, index_col=0)\n",
    "time = pd.to_datetime(occ_df.index)\n",
    "\n",
    "# Normalize occupancy by number of charging piles\n",
    "charge_count_dict = dict(zip(inf['TAZID'].astype(str), inf['charge_count']))\n",
    "for col in occ_df.columns:\n",
    "    charge_count = charge_count_dict[col]\n",
    "    occ_df[col] = occ_df[col] / charge_count\n",
    "\n",
    "feat = np.array(occ_df)\n",
    "\n",
    "print(f\"Data shape: {feat.shape}\")\n",
    "print(f\"Date range: {time.min()} to {time.max()}\")\n",
    "print(f\"Number of stations: {feat.shape[1]}\")\n",
    "print(f\"Number of time steps: {feat.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87155c86",
   "metadata": {},
   "source": [
    "## 4. Create Dataset and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274ecf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data based on fold\n",
    "month_list = list(time.month.unique())\n",
    "fold_time = time.month.isin(month_list[0:args.fold]).sum()\n",
    "\n",
    "train_end = int(fold_time * TRAIN_RATIO)\n",
    "valid_start = train_end\n",
    "valid_end = int(valid_start + fold_time * VAL_RATIO)\n",
    "test_start = valid_end\n",
    "test_end = int(fold_time)\n",
    "\n",
    "train_feat = feat[:train_end]\n",
    "valid_feat = feat[valid_start:valid_end]\n",
    "test_feat = feat[test_start:test_end]\n",
    "\n",
    "print(f\"Data split:\")\n",
    "print(f\"  Training: {train_feat.shape} (indices 0 to {train_end})\")\n",
    "print(f\"  Validation: {valid_feat.shape} (indices {valid_start} to {valid_end})\")\n",
    "print(f\"  Test: {test_feat.shape} (indices {test_start} to {test_end})\")\n",
    "print(f\"\\nTime ranges:\")\n",
    "print(f\"  Training: {time[0]} to {time[train_end-1]}\")\n",
    "print(f\"  Validation: {time[valid_start]} to {time[valid_end-1]}\")\n",
    "print(f\"  Test: {time[test_start]} to {time[test_end-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e9e5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to create sequences for RNN (matches original UrbanEV implementation)\n",
    "def create_rnn_data(dataset, lookback, predict_time):\n",
    "\t\"\"\"\n",
    "\tCreate sequences for RNN/LSTM training\n",
    "\tArgs:\n",
    "\t\tdataset: numpy array of shape (timesteps, stations)\n",
    "\t\tlookback: length of input sequence\n",
    "\t\tpredict_time: prediction horizon\n",
    "\tReturns:\n",
    "\t\tX: input sequences of shape (num_samples, seq_len, stations)\n",
    "\t\ty: target values of shape (num_samples, stations)\n",
    "\t\"\"\"\n",
    "\tx = []\n",
    "\ty = []\n",
    "\tfor i in range(len(dataset) - lookback - predict_time):\n",
    "\t\tx.append(dataset[i:i + lookback])\n",
    "\t\ty.append(dataset[i + lookback + predict_time - 1])\n",
    "\treturn np.array(x), np.array(y)\n",
    "\n",
    "# Create sequences\n",
    "train_occ, train_label = create_rnn_data(train_feat, args.seq_len, args.pred_len)\n",
    "valid_occ, valid_label = create_rnn_data(valid_feat, args.seq_len, args.pred_len)\n",
    "test_occ, test_label = create_rnn_data(test_feat, args.seq_len, args.pred_len)\n",
    "\n",
    "print(f\"Sequence data shapes:\")\n",
    "print(f\"  Training: X={train_occ.shape}, y={train_label.shape}\")\n",
    "print(f\"  Validation: X={valid_occ.shape}, y={valid_label.shape}\")\n",
    "print(f\"  Test: X={test_occ.shape}, y={test_label.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d34581e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Dataset class (matches original UrbanEV implementation)\n",
    "class CreateDataset(Dataset):\n",
    "\tdef __init__(self, occ, label, device):\n",
    "\t\tself.occ = torch.FloatTensor(occ)\n",
    "\t\tself.label = torch.FloatTensor(label)\n",
    "\t\tself.device = device\n",
    "\t\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.occ)\n",
    "\t\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\t# occ shape: (batch, seq, node) -> transpose to (batch, node, seq)\n",
    "\t\toutput_occ = torch.transpose(self.occ[idx, :, :], 0, 1).to(self.device)\n",
    "\t\toutput_label = self.label[idx, :].to(self.device)\n",
    "\t\treturn output_occ, output_label\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = CreateDataset(train_occ, train_label, device)\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.bs, shuffle=True, drop_last=True)\n",
    "\n",
    "valid_dataset = CreateDataset(valid_occ, valid_label, device)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=len(valid_occ), shuffle=False)\n",
    "\n",
    "test_dataset = CreateDataset(test_occ, test_label, device)\n",
    "test_loader = DataLoader(test_dataset, batch_size=len(test_occ), shuffle=False)\n",
    "\n",
    "print(f\"DataLoaders created:\")\n",
    "print(f\"  Training batches: {len(train_loader)}\")\n",
    "print(f\"  Validation batches: {len(valid_loader)}\")\n",
    "print(f\"  Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84f7e97",
   "metadata": {},
   "source": [
    "## 5. Initialize Model and Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9623b664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "n_fea = 1  # Only occupancy feature\n",
    "num_nodes = feat.shape[1]\n",
    "\n",
    "model = Lstm(seq=args.seq_len, n_fea=n_fea, node=num_nodes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.00001)\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "print(f\"Model initialized:\")\n",
    "print(f\"  Number of stations: {num_nodes}\")\n",
    "print(f\"  Sequence length: {args.seq_len}\")\n",
    "print(f\"  Number of features: {n_fea}\")\n",
    "print(f\"  LSTM hidden dimension: {model.lstm_hidden_dim}\")\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409d36d3",
   "metadata": {},
   "source": [
    "## 6. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37696a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in tqdm(range(args.epoch), desc='Training'):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    epoch_train_loss = 0\n",
    "    for batch_idx, (occupancy, label) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        predict = model(occupancy, None)\n",
    "        loss = loss_func(predict, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = epoch_train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for occupancy, label in valid_loader:\n",
    "            predict = model(occupancy, None)\n",
    "            valid_loss = loss_func(predict, label)\n",
    "            valid_losses.append(valid_loss.item())\n",
    "            \n",
    "            # Save best model\n",
    "            if valid_loss.item() < best_valid_loss:\n",
    "                best_valid_loss = valid_loss.item()\n",
    "                best_model_state = model.state_dict().copy()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{args.epoch} - Train Loss: {avg_train_loss:.6f}, Valid Loss: {valid_loss.item():.6f}\")\n",
    "\n",
    "print(f\"\\nTraining completed!\")\n",
    "print(f\"Best validation loss: {best_valid_loss:.6f}\")\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(best_model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30da60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.plot(train_losses, label='Training Loss', alpha=0.7, linewidth=2)\n",
    "ax.plot(range(0, len(valid_losses) * len(train_loader), len(train_loader)), \n",
    "        valid_losses, label='Validation Loss', alpha=0.7, linewidth=2, marker='o')\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Loss (MSE)')\n",
    "ax.set_title('Training and Validation Loss')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e51b15a",
   "metadata": {},
   "source": [
    "## 7. Test Model and Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5483b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "model.eval()\n",
    "predictions_list = []\n",
    "labels_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for occupancy, label in test_loader:\n",
    "        predict = model(occupancy, None)\n",
    "        predictions_list.append(predict.cpu().numpy())\n",
    "        labels_list.append(label.cpu().numpy())\n",
    "\n",
    "predictions = np.concatenate(predictions_list, axis=0)\n",
    "test_labels = np.concatenate(labels_list, axis=0)\n",
    "\n",
    "print(f\"Predictions shape: {predictions.shape}\")\n",
    "print(f\"Test labels shape: {test_labels.shape}\")\n",
    "print(f\"Predictions range: [{predictions.min():.4f}, {predictions.max():.4f}]\")\n",
    "print(f\"Actual values range: [{test_labels.min():.4f}, {test_labels.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe9df17",
   "metadata": {},
   "source": [
    "## 8. Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589f12d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"Calculate various regression metrics\"\"\"\n",
    "    eps = 2e-2\n",
    "    \n",
    "    # Handle near-zero values for MAPE\n",
    "    y_true_mape = y_true.copy()\n",
    "    y_pred_mape = y_pred.copy()\n",
    "    y_true_mape[np.where(y_true_mape <= eps)] = np.abs(y_true_mape[np.where(y_true_mape <= eps)]) + eps\n",
    "    y_pred_mape[np.where(y_true_mape <= eps)] = np.abs(y_pred_mape[np.where(y_true_mape <= eps)]) + eps\n",
    "    \n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_true_mape, y_pred_mape)\n",
    "    rae = np.sum(np.abs(y_pred_mape - y_true_mape)) / np.sum(np.abs(np.mean(y_true_mape) - y_true_mape))\n",
    "    \n",
    "    return {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'MAPE': mape,\n",
    "        'RAE': rae\n",
    "    }\n",
    "\n",
    "# Calculate overall metrics\n",
    "metrics = calculate_metrics(test_labels, predictions)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LSTM MODEL PERFORMANCE - ALL STATIONS\")\n",
    "print(\"=\" * 60)\n",
    "for metric_name, metric_value in metrics.items():\n",
    "    print(f\"{metric_name:10s}: {metric_value:.6f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9749ec45",
   "metadata": {},
   "source": [
    "## 9. Visualize Results - Overall Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745e34e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs predicted scatter for all stations\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Scatter plot\n",
    "axes[0].scatter(test_labels.flatten(), predictions.flatten(), \n",
    "               alpha=0.3, s=1, c='steelblue')\n",
    "axes[0].plot([test_labels.min(), test_labels.max()], \n",
    "            [test_labels.min(), test_labels.max()], \n",
    "            'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[0].set_xlabel('Actual Occupancy (Normalized)')\n",
    "axes[0].set_ylabel('Predicted Occupancy (Normalized)')\n",
    "axes[0].set_title('Actual vs Predicted - All Stations (LSTM)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals histogram\n",
    "residuals = test_labels.flatten() - predictions.flatten()\n",
    "axes[1].hist(residuals, bins=50, edgecolor='black', alpha=0.7, color='coral')\n",
    "axes[1].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Residuals (Actual - Predicted)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Distribution of Residuals')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean residual: {residuals.mean():.6f}\")\n",
    "print(f\"Std residual: {residuals.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fc0ef8",
   "metadata": {},
   "source": [
    "## 10. Visualize Results - Time Series for Selected Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74dcd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a few representative stations to visualize\n",
    "selected_stations = [0, 50, 100, 150, 200, 250]  # Indices of stations to plot\n",
    "station_ids = occ_df.columns[selected_stations]\n",
    "\n",
    "# Time indices for test data (accounting for sequence creation)\n",
    "test_time = time[test_start + args.seq_len + args.pred_len - 1:test_end - 1]\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (station_idx, station_id) in enumerate(zip(selected_stations, station_ids)):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Plot actual vs predicted\n",
    "    ax.plot(test_time[:len(test_labels)], test_labels[:, station_idx], \n",
    "           label='Actual', linewidth=1.5, alpha=0.8, color='steelblue')\n",
    "    ax.plot(test_time[:len(predictions)], predictions[:, station_idx], \n",
    "           label='Predicted', linewidth=1.5, alpha=0.8, color='coral', linestyle='--')\n",
    "    \n",
    "    # Calculate station-specific metrics\n",
    "    station_mae = mean_absolute_error(test_labels[:, station_idx], predictions[:, station_idx])\n",
    "    \n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Occupancy (Normalized)')\n",
    "    ax.set_title(f'Station {station_id} - MAE: {station_mae:.4f}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d2e4f5",
   "metadata": {},
   "source": [
    "## 11. Per-Station Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323402a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for each station\n",
    "station_metrics = []\n",
    "\n",
    "for station_idx in range(feat.shape[1]):\n",
    "    station_id = occ_df.columns[station_idx]\n",
    "    station_mae = mean_absolute_error(test_labels[:, station_idx], \n",
    "                                     predictions[:, station_idx])\n",
    "    station_rmse = np.sqrt(mean_squared_error(test_labels[:, station_idx], \n",
    "                                              predictions[:, station_idx]))\n",
    "    \n",
    "    station_metrics.append({\n",
    "        'Station_ID': station_id,\n",
    "        'MAE': station_mae,\n",
    "        'RMSE': station_rmse\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(station_metrics)\n",
    "\n",
    "print(\"Per-Station Performance Summary:\")\n",
    "print(metrics_df.describe())\n",
    "print(f\"\\nTop 10 Best Performing Stations (Lowest MAE):\")\n",
    "print(metrics_df.nsmallest(10, 'MAE'))\n",
    "print(f\"\\nTop 10 Worst Performing Stations (Highest MAE):\")\n",
    "print(metrics_df.nlargest(10, 'MAE'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f60fd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distribution of station-level performance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# MAE distribution\n",
    "axes[0].hist(metrics_df['MAE'], bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "axes[0].axvline(x=metrics_df['MAE'].mean(), color='red', linestyle='--', \n",
    "               linewidth=2, label=f'Mean: {metrics_df[\"MAE\"].mean():.4f}')\n",
    "axes[0].set_xlabel('MAE')\n",
    "axes[0].set_ylabel('Number of Stations')\n",
    "axes[0].set_title('Distribution of MAE Across All Stations (LSTM)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# RMSE distribution\n",
    "axes[1].hist(metrics_df['RMSE'], bins=30, edgecolor='black', alpha=0.7, color='coral')\n",
    "axes[1].axvline(x=metrics_df['RMSE'].mean(), color='red', linestyle='--', \n",
    "               linewidth=2, label=f'Mean: {metrics_df[\"RMSE\"].mean():.4f}')\n",
    "axes[1].set_xlabel('RMSE')\n",
    "axes[1].set_ylabel('Number of Stations')\n",
    "axes[1].set_title('Distribution of RMSE Across All Stations (LSTM)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27714e1",
   "metadata": {},
   "source": [
    "## 12. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4096e25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"LSTM MODEL - FINAL SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nModel Configuration:\")\n",
    "print(f\"  Prediction Horizon: {args.pred_len} hour(s)\")\n",
    "print(f\"  Look-back Window: {args.seq_len} hours\")\n",
    "print(f\"  Number of Stations: {feat.shape[1]}\")\n",
    "print(f\"  Test Set Size: {test_labels.shape[0]} time steps\")\n",
    "print(f\"  LSTM Hidden Dim: {model.lstm_hidden_dim}\")\n",
    "print(f\"  LSTM Layers: 2\")\n",
    "print(f\"  Training Epochs: {args.epoch}\")\n",
    "print(f\"  Batch Size: {args.bs}\")\n",
    "\n",
    "print(f\"\\nOverall Performance (All Stations):\")\n",
    "for metric_name, metric_value in metrics.items():\n",
    "    print(f\"  {metric_name:10s}: {metric_value:.6f}\")\n",
    "\n",
    "print(f\"\\nPer-Station Performance Statistics:\")\n",
    "print(f\"  MAE - Mean: {metrics_df['MAE'].mean():.6f}, Std: {metrics_df['MAE'].std():.6f}\")\n",
    "print(f\"  MAE - Min:  {metrics_df['MAE'].min():.6f}, Max: {metrics_df['MAE'].max():.6f}\")\n",
    "print(f\"  RMSE - Mean: {metrics_df['RMSE'].mean():.6f}, Std: {metrics_df['RMSE'].std():.6f}\")\n",
    "print(f\"  RMSE - Min:  {metrics_df['RMSE'].min():.6f}, Max: {metrics_df['RMSE'].max():.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"The LSTM model uses recurrent neural networks to learn temporal patterns\")\n",
    "print(\"in the charging station occupancy data. It processes sequences of past\")\n",
    "print(\"observations to predict future values, capturing complex time dependencies.\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
