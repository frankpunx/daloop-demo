{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d6dbca9",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4628825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('UrbanEV-main/code')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ea4eae",
   "metadata": {},
   "source": [
    "## 2. Load Data and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ab0cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.feat = 'occ'  # Feature type: occupancy\n",
    "        self.pred_len = 3  # Prediction horizon (1 hour ahead)\n",
    "        self.seq_len = 12  # Look-back window (12 hours)\n",
    "        self.add_feat = 'None'  # No additional features\n",
    "        self.pred_type = 'region'  # Predict all regions/stations\n",
    "        self.fold = 6  # Cross-validation fold\n",
    "        self.total_fold = 6  # Total number of folds (months)\n",
    "        self.seed = 42\n",
    "\n",
    "args = Args()\n",
    "\n",
    "# Train-validation-test split ratios\n",
    "TRAIN_RATIO, VAL_RATIO, TEST_RATIO = 0.8, 0.1, 0.1\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Feature: {args.feat}\")\n",
    "print(f\"  Prediction horizon: {args.pred_len} hour(s)\")\n",
    "print(f\"  Look-back window: {args.seq_len} hours\")\n",
    "print(f\"  Prediction type: {args.pred_type}\")\n",
    "print(f\"  Train/Val/Test ratio: {TRAIN_RATIO}/{VAL_RATIO}/{TEST_RATIO}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c43fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load station information\n",
    "inf = pd.read_csv('UrbanEV-main/data/inf.csv', header=0, index_col=None)\n",
    "print(f\"Total stations: {len(inf)}\")\n",
    "print(f\"\\nStation information:\")\n",
    "print(inf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac500ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load occupancy data\n",
    "occ_df = pd.read_csv('UrbanEV-main/data/occupancy.csv', header=0, index_col=0)\n",
    "time = pd.to_datetime(occ_df.index)\n",
    "\n",
    "# Normalize occupancy by number of charging piles\n",
    "charge_count_dict = dict(zip(inf['TAZID'].astype(str), inf['charge_count']))\n",
    "for col in occ_df.columns:\n",
    "    charge_count = charge_count_dict[col]\n",
    "    occ_df[col] = occ_df[col] / charge_count\n",
    "\n",
    "feat = np.array(occ_df)\n",
    "\n",
    "print(f\"Data shape: {feat.shape}\")\n",
    "print(f\"Date range: {time.min()} to {time.max()}\")\n",
    "print(f\"Number of stations: {feat.shape[1]}\")\n",
    "print(f\"Number of time steps: {feat.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd321c9d",
   "metadata": {},
   "source": [
    "## 3. Split Data (Train/Validation/Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35220155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data based on fold\n",
    "month_list = list(time.month.unique())\n",
    "fold_time = time.month.isin(month_list[0:args.fold]).sum()\n",
    "\n",
    "train_end = int(fold_time * TRAIN_RATIO)\n",
    "valid_start = train_end\n",
    "valid_end = int(valid_start + fold_time * VAL_RATIO)\n",
    "test_start = valid_end\n",
    "test_end = int(fold_time)\n",
    "\n",
    "train_feat = feat[:train_end]\n",
    "valid_feat = feat[valid_start:valid_end]\n",
    "test_feat = feat[test_start:test_end]\n",
    "\n",
    "print(f\"Data split:\")\n",
    "print(f\"  Training: {train_feat.shape} (indices 0 to {train_end})\")\n",
    "print(f\"  Validation: {valid_feat.shape} (indices {valid_start} to {valid_end})\")\n",
    "print(f\"  Test: {test_feat.shape} (indices {test_start} to {test_end})\")\n",
    "print(f\"\\nTime ranges:\")\n",
    "print(f\"  Training: {time[0]} to {time[train_end-1]}\")\n",
    "print(f\"  Validation: {time[valid_start]} to {time[valid_end-1]}\")\n",
    "print(f\"  Test: {time[test_start]} to {time[test_end-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caffa450",
   "metadata": {},
   "source": [
    "## 4. Initialize and Run Lo Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf173d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lo model implementation\n",
    "class Lo:\n",
    "    def __init__(self, pred_len):\n",
    "        self.pred_len = pred_len\n",
    "\n",
    "    def predict(self, train_valid_occ, test_occ):\n",
    "        \"\"\"\n",
    "        Use the latest observed value as the prediction for the next time step.\n",
    "        For each prediction, use the value from pred_len steps before.\n",
    "        \"\"\"\n",
    "        time_len, node = test_occ.shape\n",
    "        preds = np.zeros((time_len, node))\n",
    "\n",
    "        for j in range(node):\n",
    "            for i in range(time_len):\n",
    "                if i < self.pred_len:\n",
    "                    # Use from training/validation data\n",
    "                    preds[i, j] = train_valid_occ[-self.pred_len + i, j]\n",
    "                else:\n",
    "                    # Use from test data (actual observations)\n",
    "                    preds[i, j] = test_occ[i - self.pred_len, j]\n",
    "\n",
    "        return preds\n",
    "\n",
    "# Initialize model\n",
    "lo_model = Lo(pred_len=args.pred_len)\n",
    "print(f\"Lo model initialized with prediction horizon: {args.pred_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cd2aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for Lo model\n",
    "# Combine train and valid data for the model's reference\n",
    "train_valid_feat = np.vstack((train_feat, valid_feat, test_feat[:args.seq_len + args.pred_len, :]))\n",
    "test_feat_adjusted = test_feat[args.pred_len + args.seq_len:, :]\n",
    "\n",
    "print(f\"Train+Valid data shape: {train_valid_feat.shape}\")\n",
    "print(f\"Test data shape (adjusted): {test_feat_adjusted.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3424fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = lo_model.predict(train_valid_feat, test_feat_adjusted)\n",
    "\n",
    "print(f\"Predictions shape: {predictions.shape}\")\n",
    "print(f\"Predictions range: [{predictions.min():.4f}, {predictions.max():.4f}]\")\n",
    "print(f\"Actual values range: [{test_feat_adjusted.min():.4f}, {test_feat_adjusted.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ec1e7e",
   "metadata": {},
   "source": [
    "## 5. Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f753debd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "# Calculate metrics\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"Calculate various regression metrics\"\"\"\n",
    "    eps = 2e-2\n",
    "    \n",
    "    # Handle near-zero values for MAPE\n",
    "    y_true_mape = y_true.copy()\n",
    "    y_pred_mape = y_pred.copy()\n",
    "    y_true_mape[np.where(y_true_mape <= eps)] = np.abs(y_true_mape[np.where(y_true_mape <= eps)]) + eps\n",
    "    y_pred_mape[np.where(y_true_mape <= eps)] = np.abs(y_pred_mape[np.where(y_true_mape <= eps)]) + eps\n",
    "    \n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_true_mape, y_pred_mape)\n",
    "    rae = np.sum(np.abs(y_pred_mape - y_true_mape)) / np.sum(np.abs(np.mean(y_true_mape) - y_true_mape))\n",
    "    \n",
    "    return {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'MAPE': mape,\n",
    "        'RAE': rae\n",
    "    }\n",
    "\n",
    "# Calculate overall metrics\n",
    "metrics = calculate_metrics(test_feat_adjusted, predictions)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LO MODEL PERFORMANCE - ALL STATIONS\")\n",
    "print(\"=\" * 60)\n",
    "for metric_name, metric_value in metrics.items():\n",
    "    print(f\"{metric_name:10s}: {metric_value:.6f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9754e46",
   "metadata": {},
   "source": [
    "## 6. Visualize Results - Overall Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52c0d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs predicted scatter for all stations\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Scatter plot\n",
    "axes[0].scatter(test_feat_adjusted.flatten(), predictions.flatten(), \n",
    "               alpha=0.3, s=1, c='steelblue')\n",
    "axes[0].plot([test_feat_adjusted.min(), test_feat_adjusted.max()], \n",
    "            [test_feat_adjusted.min(), test_feat_adjusted.max()], \n",
    "            'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[0].set_xlabel('Actual Occupancy (Normalized)')\n",
    "axes[0].set_ylabel('Predicted Occupancy (Normalized)')\n",
    "axes[0].set_title('Actual vs Predicted - All Stations')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals histogram\n",
    "residuals = test_feat_adjusted.flatten() - predictions.flatten()\n",
    "axes[1].hist(residuals, bins=50, edgecolor='black', alpha=0.7, color='coral')\n",
    "axes[1].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Residuals (Actual - Predicted)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Distribution of Residuals')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean residual: {residuals.mean():.6f}\")\n",
    "print(f\"Std residual: {residuals.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f0d0de",
   "metadata": {},
   "source": [
    "## 7. Visualize Results - Time Series for Selected Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bace3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a few representative stations to visualize\n",
    "selected_stations = [0, 50, 100, 150, 200, 250]  # Indices of stations to plot\n",
    "station_ids = occ_df.columns[selected_stations]\n",
    "\n",
    "# Time indices for test data\n",
    "test_time = time[test_start + args.pred_len + args.seq_len:test_end]\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (station_idx, station_id) in enumerate(zip(selected_stations, station_ids)):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Plot actual vs predicted\n",
    "    ax.plot(test_time, test_feat_adjusted[:, station_idx], \n",
    "           label='Actual', linewidth=1.5, alpha=0.8, color='steelblue')\n",
    "    ax.plot(test_time, predictions[:, station_idx], \n",
    "           label='Predicted', linewidth=1.5, alpha=0.8, color='coral', linestyle='--')\n",
    "    \n",
    "    # Calculate station-specific metrics\n",
    "    station_mae = mean_absolute_error(test_feat_adjusted[:, station_idx], predictions[:, station_idx])\n",
    "    \n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Occupancy (Normalized)')\n",
    "    ax.set_title(f'Station {station_id} - MAE: {station_mae:.4f}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d128713f",
   "metadata": {},
   "source": [
    "## 8. Per-Station Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1c7bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for each station\n",
    "station_metrics = []\n",
    "\n",
    "for station_idx in range(feat.shape[1]):\n",
    "    station_id = occ_df.columns[station_idx]\n",
    "    station_mae = mean_absolute_error(test_feat_adjusted[:, station_idx], \n",
    "                                     predictions[:, station_idx])\n",
    "    station_rmse = np.sqrt(mean_squared_error(test_feat_adjusted[:, station_idx], \n",
    "                                              predictions[:, station_idx]))\n",
    "    \n",
    "    station_metrics.append({\n",
    "        'Station_ID': station_id,\n",
    "        'MAE': station_mae,\n",
    "        'RMSE': station_rmse\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(station_metrics)\n",
    "\n",
    "print(\"Per-Station Performance Summary:\")\n",
    "print(metrics_df.describe())\n",
    "print(f\"\\nTop 10 Best Performing Stations (Lowest MAE):\")\n",
    "print(metrics_df.nsmallest(10, 'MAE'))\n",
    "print(f\"\\nTop 10 Worst Performing Stations (Highest MAE):\")\n",
    "print(metrics_df.nlargest(10, 'MAE'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b15961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distribution of station-level performance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# MAE distribution\n",
    "axes[0].hist(metrics_df['MAE'], bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "axes[0].axvline(x=metrics_df['MAE'].mean(), color='red', linestyle='--', \n",
    "               linewidth=2, label=f'Mean: {metrics_df[\"MAE\"].mean():.4f}')\n",
    "axes[0].set_xlabel('MAE')\n",
    "axes[0].set_ylabel('Number of Stations')\n",
    "axes[0].set_title('Distribution of MAE Across All Stations')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# RMSE distribution\n",
    "axes[1].hist(metrics_df['RMSE'], bins=30, edgecolor='black', alpha=0.7, color='coral')\n",
    "axes[1].axvline(x=metrics_df['RMSE'].mean(), color='red', linestyle='--', \n",
    "               linewidth=2, label=f'Mean: {metrics_df[\"RMSE\"].mean():.4f}')\n",
    "axes[1].set_xlabel('RMSE')\n",
    "axes[1].set_ylabel('Number of Stations')\n",
    "axes[1].set_title('Distribution of RMSE Across All Stations')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6ac14a",
   "metadata": {},
   "source": [
    "## 9. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4767ba6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"LO MODEL - FINAL SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nModel Configuration:\")\n",
    "print(f\"  Prediction Horizon: {args.pred_len} hour(s)\")\n",
    "print(f\"  Look-back Window: {args.seq_len} hours\")\n",
    "print(f\"  Number of Stations: {feat.shape[1]}\")\n",
    "print(f\"  Test Set Size: {test_feat_adjusted.shape[0]} time steps\")\n",
    "\n",
    "print(f\"\\nOverall Performance (All Stations):\")\n",
    "for metric_name, metric_value in metrics.items():\n",
    "    print(f\"  {metric_name:10s}: {metric_value:.6f}\")\n",
    "\n",
    "print(f\"\\nPer-Station Performance Statistics:\")\n",
    "print(f\"  MAE - Mean: {metrics_df['MAE'].mean():.6f}, Std: {metrics_df['MAE'].std():.6f}\")\n",
    "print(f\"  MAE - Min:  {metrics_df['MAE'].min():.6f}, Max: {metrics_df['MAE'].max():.6f}\")\n",
    "print(f\"  RMSE - Mean: {metrics_df['RMSE'].mean():.6f}, Std: {metrics_df['RMSE'].std():.6f}\")\n",
    "print(f\"  RMSE - Min:  {metrics_df['RMSE'].min():.6f}, Max: {metrics_df['RMSE'].max():.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"The Lo model serves as a simple baseline that uses the most recent\")\n",
    "print(\"observation as the prediction. Despite its simplicity, it often performs\")\n",
    "print(\"well for short-term forecasting in stable time series.\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
