{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17689574",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71407e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94b9fa6",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad5f724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the station dataset\n",
    "df = pd.read_csv('station_1068_dataset.csv')\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "df.set_index('time', inplace=True)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Date range: {df.index.min()} to {df.index.max()}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0001c334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values:\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"\\nData types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6ced5c",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a808d153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for modeling\n",
    "feature_columns = ['occupancy', 'e_price', 's_price', \n",
    "                   'T', 'P0', 'U', 'nRAIN', 'Td',\n",
    "                   'hour', 'day_of_week', 'is_weekend']\n",
    "\n",
    "df_model = df[feature_columns].copy()\n",
    "\n",
    "print(f\"Selected features: {len(feature_columns)}\")\n",
    "print(f\"Dataset shape: {df_model.shape}\")\n",
    "print(f\"\\nSummary statistics:\")\n",
    "print(df_model.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41605e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train, validation, and test sets\n",
    "n = len(df_model)\n",
    "train_df = df_model[0:int(n*0.7)]\n",
    "val_df = df_model[int(n*0.7):int(n*0.9)]\n",
    "test_df = df_model[int(n*0.9):]\n",
    "\n",
    "print(f\"Training set: {len(train_df)} samples ({len(train_df)/n*100:.1f}%)\")\n",
    "print(f\"Validation set: {len(val_df)} samples ({len(val_df)/n*100:.1f}%)\")\n",
    "print(f\"Test set: {len(test_df)} samples ({len(test_df)/n*100:.1f}%)\")\n",
    "print(f\"\\nTime ranges:\")\n",
    "print(f\"  Train: {train_df.index[0]} to {train_df.index[-1]}\")\n",
    "print(f\"  Val: {val_df.index[0]} to {val_df.index[-1]}\")\n",
    "print(f\"  Test: {test_df.index[0]} to {test_df.index[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ce8a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "train_mean = train_df.mean()\n",
    "train_std = train_df.std()\n",
    "\n",
    "train_df = (train_df - train_mean) / train_std\n",
    "val_df = (val_df - train_mean) / train_std\n",
    "test_df = (test_df - train_mean) / train_std\n",
    "\n",
    "print(\"Data normalized using training set statistics\")\n",
    "print(f\"\\nNormalized training data:\")\n",
    "print(train_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daba0f83",
   "metadata": {},
   "source": [
    "## 4. Create WindowGenerator Class\n",
    "\n",
    "This class handles the creation of time series windows for training and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2abc80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowGenerator:\n",
    "    def __init__(self, input_width, label_width, shift,\n",
    "                 train_df, val_df, test_df,\n",
    "                 label_columns=None):\n",
    "        # Store the raw data\n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "        self.test_df = test_df\n",
    "\n",
    "        # Work out the label column indices\n",
    "        self.label_columns = label_columns\n",
    "        if label_columns is not None:\n",
    "            self.label_columns_indices = {name: i for i, name in enumerate(label_columns)}\n",
    "        self.column_indices = {name: i for i, name in enumerate(train_df.columns)}\n",
    "\n",
    "        # Work out the window parameters\n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "\n",
    "        self.total_window_size = input_width + shift\n",
    "\n",
    "        self.input_slice = slice(0, input_width)\n",
    "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "        self.label_start = self.total_window_size - self.label_width\n",
    "        self.labels_slice = slice(self.label_start, None)\n",
    "        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([\n",
    "            f'Total window size: {self.total_window_size}',\n",
    "            f'Input indices: {self.input_indices}',\n",
    "            f'Label indices: {self.label_indices}',\n",
    "            f'Label column name(s): {self.label_columns}'])\n",
    "\n",
    "    def split_window(self, features):\n",
    "        inputs = features[:, self.input_slice, :]\n",
    "        labels = features[:, self.labels_slice, :]\n",
    "        if self.label_columns is not None:\n",
    "            labels = tf.stack(\n",
    "                [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
    "                axis=-1)\n",
    "\n",
    "        # Slicing doesn't preserve static shape information, so set the shapes\n",
    "        inputs.set_shape([None, self.input_width, None])\n",
    "        labels.set_shape([None, self.label_width, None])\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "    def make_dataset(self, data):\n",
    "        data = np.array(data, dtype=np.float32)\n",
    "        ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "            data=data,\n",
    "            targets=None,\n",
    "            sequence_length=self.total_window_size,\n",
    "            sequence_stride=1,\n",
    "            shuffle=True,\n",
    "            batch_size=32,)\n",
    "\n",
    "        ds = ds.map(self.split_window)\n",
    "        return ds\n",
    "\n",
    "    @property\n",
    "    def train(self):\n",
    "        return self.make_dataset(self.train_df)\n",
    "\n",
    "    @property\n",
    "    def val(self):\n",
    "        return self.make_dataset(self.val_df)\n",
    "\n",
    "    @property\n",
    "    def test(self):\n",
    "        return self.make_dataset(self.test_df)\n",
    "\n",
    "    @property\n",
    "    def example(self):\n",
    "        \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
    "        result = getattr(self, '_example', None)\n",
    "        if result is None:\n",
    "            # No example batch was found, so get one from the `.train` dataset\n",
    "            result = next(iter(self.train))\n",
    "            # And cache it for next time\n",
    "            self._example = result\n",
    "        return result\n",
    "\n",
    "print(\"WindowGenerator class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80ea049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the WindowGenerator\n",
    "# Example: Use 24 hours of data to predict next 12 hours\n",
    "w1 = WindowGenerator(input_width=24, label_width=12, shift=12,\n",
    "                     train_df=train_df, val_df=val_df, test_df=test_df,\n",
    "                     label_columns=['occupancy'])\n",
    "\n",
    "print(w1)\n",
    "print(f\"\\nExample batch shapes:\")\n",
    "example_inputs, example_labels = w1.example\n",
    "print(f\"  Inputs shape: {example_inputs.shape}\")\n",
    "print(f\"  Labels shape: {example_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2461cd",
   "metadata": {},
   "source": [
    "## 5. Create Plotting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9e6ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(window, model, plot_col='occupancy', max_subplots=3):\n",
    "    \"\"\"Plot model predictions vs actual values\"\"\"\n",
    "    inputs, labels = window.example\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plot_col_index = window.column_indices[plot_col]\n",
    "    max_n = min(max_subplots, len(inputs))\n",
    "    \n",
    "    for n in range(max_n):\n",
    "        plt.subplot(max_n, 1, n+1)\n",
    "        plt.ylabel(f'{plot_col} [normed]')\n",
    "        plt.plot(window.input_indices, inputs[n, :, plot_col_index],\n",
    "                 label='Inputs', marker='.', zorder=-10)\n",
    "\n",
    "        if window.label_columns:\n",
    "            label_col_index = window.label_columns_indices.get(plot_col, None)\n",
    "        else:\n",
    "            label_col_index = plot_col_index\n",
    "\n",
    "        if label_col_index is None:\n",
    "            continue\n",
    "\n",
    "        plt.scatter(window.label_indices, labels[n, :, label_col_index],\n",
    "                    edgecolors='k', label='Labels', c='#2ca02c', s=64)\n",
    "        \n",
    "        if model is not None:\n",
    "            predictions = model(inputs)\n",
    "            plt.scatter(window.label_indices, predictions[n, :, label_col_index],\n",
    "                        marker='X', edgecolors='k', label='Predictions',\n",
    "                        c='#ff7f0e', s=64)\n",
    "\n",
    "        if n == 0:\n",
    "            plt.legend()\n",
    "\n",
    "    plt.xlabel('Time [h]')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Plotting function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ea3534",
   "metadata": {},
   "source": [
    "## 6. Build Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eff984f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline 1: Last value (persistence)\n",
    "class Baseline(tf.keras.Model):\n",
    "    def __init__(self, label_index=None):\n",
    "        super().__init__()\n",
    "        self.label_index = label_index\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if self.label_index is None:\n",
    "            return inputs\n",
    "        # Take the last value from input and repeat it for the prediction window\n",
    "        result = inputs[:, -1:, self.label_index]\n",
    "        return tf.tile(result[:, :, tf.newaxis], [1, 12, 1])\n",
    "\n",
    "# Create and test baseline\n",
    "baseline = Baseline(label_index=w1.column_indices['occupancy'])\n",
    "\n",
    "baseline.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "                 metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
    "\n",
    "val_performance = {}\n",
    "performance = {}\n",
    "\n",
    "val_performance['Baseline'] = baseline.evaluate(w1.val, verbose=0)\n",
    "performance['Baseline'] = baseline.evaluate(w1.test, verbose=0)\n",
    "\n",
    "print(\"Baseline Model (Last Value Persistence):\")\n",
    "print(f\"  Validation - Loss: {val_performance['Baseline'][0]:.4f}, MAE: {val_performance['Baseline'][1]:.4f}\")\n",
    "print(f\"  Test - Loss: {performance['Baseline'][0]:.4f}, MAE: {performance['Baseline'][1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86e123e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize baseline predictions\n",
    "plot_predictions(w1, baseline, plot_col='occupancy', max_subplots=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7309d4",
   "metadata": {},
   "source": [
    "## 7. Build Dense (Fully Connected) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594d2640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense model with multiple layers\n",
    "# Flatten input, pass through dense layers, reshape to match label_width\n",
    "dense_model = tf.keras.Sequential([\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(units=64, activation='relu'),\n",
    "    layers.Dense(units=64, activation='relu'),\n",
    "    layers.Dense(units=w1.label_width),  # Output for 12 time steps\n",
    "    layers.Reshape([w1.label_width, 1])  # Reshape to (batch, 12, 1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "dense_model.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "                    optimizer=tf.keras.optimizers.Adam(),\n",
    "                    metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=30,\n",
    "    mode='min',\n",
    "    restore_best_weights=True)\n",
    "\n",
    "print(\"Dense model created\")\n",
    "print(dense_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88504b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dense model\n",
    "history_dense = dense_model.fit(\n",
    "    w1.train,\n",
    "    epochs=50,\n",
    "    validation_data=w1.val,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1)\n",
    "\n",
    "# Evaluate\n",
    "val_performance['Dense'] = dense_model.evaluate(w1.val, verbose=0)\n",
    "performance['Dense'] = dense_model.evaluate(w1.test, verbose=0)\n",
    "\n",
    "print(f\"\\nDense Model:\")\n",
    "print(f\"  Validation - Loss: {val_performance['Dense'][0]:.4f}, MAE: {val_performance['Dense'][1]:.4f}\")\n",
    "print(f\"  Test - Loss: {performance['Dense'][0]:.4f}, MAE: {performance['Dense'][1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0975cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the element specification of the dataset\n",
    "print(\"Training dataset element spec:\")\n",
    "print(w1.train.element_spec)\n",
    "\n",
    "# Or get shapes from a batch\n",
    "inputs, labels = next(iter(w1.train))\n",
    "print(f\"\\nBatch shapes:\")\n",
    "print(f\"  Inputs: {inputs.shape}\")\n",
    "print(f\"  Labels: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15bec6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dense_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad214a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_dense.history['loss'], label='Training Loss')\n",
    "plt.plot(history_dense.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Dense Model Training History')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_dense.history['mean_absolute_error'], label='Training MAE')\n",
    "plt.plot(history_dense.history['val_mean_absolute_error'], label='Validation MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.title('Dense Model MAE History')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625353fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize dense model predictions\n",
    "plot_predictions(w1, dense_model, plot_col='occupancy', max_subplots=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d669e40",
   "metadata": {},
   "source": [
    "## 8. Build LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c7e486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM model - outputs only the last 12 timesteps\n",
    "lstm_model = tf.keras.Sequential([\n",
    "    layers.LSTM(32, return_sequences=True),\n",
    "    layers.Dense(units=1),\n",
    "    layers.Lambda(lambda x: x[:, -w1.label_width:, :])  # Take only last 12 timesteps\n",
    "])\n",
    "\n",
    "lstm_model.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "                   optimizer=tf.keras.optimizers.Adam(),\n",
    "                   metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
    "\n",
    "print(\"LSTM model created\")\n",
    "print(lstm_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcffda91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LSTM model\n",
    "history_lstm = lstm_model.fit(\n",
    "    w1.train,\n",
    "    epochs=50,\n",
    "    validation_data=w1.val,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1)\n",
    "\n",
    "# Evaluate\n",
    "val_performance['LSTM'] = lstm_model.evaluate(w1.val, verbose=0)\n",
    "performance['LSTM'] = lstm_model.evaluate(w1.test, verbose=0)\n",
    "\n",
    "print(f\"\\nLSTM Model:\")\n",
    "print(f\"  Validation - Loss: {val_performance['LSTM'][0]:.4f}, MAE: {val_performance['LSTM'][1]:.4f}\")\n",
    "print(f\"  Test - Loss: {performance['LSTM'][0]:.4f}, MAE: {performance['LSTM'][1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bffab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot LSTM training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_lstm.history['loss'], label='Training Loss')\n",
    "plt.plot(history_lstm.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('LSTM Model Training History')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_lstm.history['mean_absolute_error'], label='Training MAE')\n",
    "plt.plot(history_lstm.history['val_mean_absolute_error'], label='Validation MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.title('LSTM Model MAE History')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a9b587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize LSTM predictions\n",
    "plot_predictions(w1, lstm_model, plot_col='occupancy', max_subplots=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95df6c9e",
   "metadata": {},
   "source": [
    "## 9. Advanced Multi-Step Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ce54d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create windows with different configurations\n",
    "# Multi-output: predict next 24 hours using last 24 hours\n",
    "multi_window = WindowGenerator(input_width=24, label_width=24, shift=24,\n",
    "                                train_df=train_df, val_df=val_df, test_df=test_df,\n",
    "                                label_columns=['occupancy'])\n",
    "\n",
    "print(\"Multi-step Window Configuration:\")\n",
    "print(multi_window)\n",
    "\n",
    "# Build multi-step LSTM\n",
    "multi_lstm_model = tf.keras.Sequential([\n",
    "    layers.LSTM(64, return_sequences=True),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.LSTM(32, return_sequences=True),\n",
    "    layers.Dense(units=1)\n",
    "])\n",
    "\n",
    "multi_lstm_model.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "                          optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                          metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
    "\n",
    "print(\"\\nMulti-step LSTM model created\")\n",
    "print(multi_lstm_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f9838b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multi-step LSTM\n",
    "history_multi = multi_lstm_model.fit(\n",
    "    multi_window.train,\n",
    "    epochs=50,\n",
    "    validation_data=multi_window.val,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1)\n",
    "\n",
    "# Evaluate\n",
    "val_performance['Multi-LSTM'] = multi_lstm_model.evaluate(multi_window.val, verbose=0)\n",
    "performance['Multi-LSTM'] = multi_lstm_model.evaluate(multi_window.test, verbose=0)\n",
    "\n",
    "print(f\"\\nMulti-step LSTM Model:\")\n",
    "print(f\"  Validation - Loss: {val_performance['Multi-LSTM'][0]:.4f}, MAE: {val_performance['Multi-LSTM'][1]:.4f}\")\n",
    "print(f\"  Test - Loss: {performance['Multi-LSTM'][0]:.4f}, MAE: {performance['Multi-LSTM'][1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e28ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot multi-step LSTM training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_multi.history['loss'], label='Training Loss')\n",
    "plt.plot(history_multi.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Multi-step LSTM Training History')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_multi.history['mean_absolute_error'], label='Training MAE')\n",
    "plt.plot(history_multi.history['val_mean_absolute_error'], label='Validation MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.title('Multi-step LSTM MAE History')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3862d83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize multi-step predictions\n",
    "plot_predictions(multi_window, multi_lstm_model, plot_col='occupancy', max_subplots=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1191460a",
   "metadata": {},
   "source": [
    "## 10. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55e4e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "x = np.arange(len(performance))\n",
    "width = 0.3\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plot test performance\n",
    "test_mae = [performance[name][1] for name in performance]\n",
    "test_loss = [performance[name][0] for name in performance]\n",
    "\n",
    "ax.bar(x - width/2, test_mae, width, label='MAE', color='steelblue', alpha=0.8)\n",
    "ax.bar(x + width/2, test_loss, width, label='MSE Loss', color='coral', alpha=0.8)\n",
    "\n",
    "ax.set_ylabel('Error')\n",
    "ax.set_title('Model Performance Comparison (Test Set)')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(performance.keys(), rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTest Set Performance Summary:\")\n",
    "print(\"=\" * 60)\n",
    "for name in performance:\n",
    "    print(f\"{name:15s} - MSE: {performance[name][0]:.4f}, MAE: {performance[name][1]:.4f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f681cd0",
   "metadata": {},
   "source": [
    "## 11. Detailed Predictions on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94fc718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on entire test set for multi-step model\n",
    "def predict_on_test_set(model, window, test_df, train_mean, train_std):\n",
    "    \"\"\"Generate predictions for entire test set and denormalize\"\"\"\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    # Get test dataset without shuffling for sequential prediction\n",
    "    test_data = np.array(window.test_df, dtype=np.float32)\n",
    "    test_ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "        data=test_data,\n",
    "        targets=None,\n",
    "        sequence_length=window.total_window_size,\n",
    "        sequence_stride=1,\n",
    "        shuffle=False,\n",
    "        batch_size=32)\n",
    "    \n",
    "    test_ds = test_ds.map(window.split_window)\n",
    "    \n",
    "    for inputs, labels in test_ds:\n",
    "        preds = model(inputs)\n",
    "        predictions.append(preds.numpy())\n",
    "        actuals.append(labels.numpy())\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    actuals = np.concatenate(actuals, axis=0)\n",
    "    \n",
    "    # Denormalize occupancy predictions\n",
    "    occ_col_idx = window.column_indices['occupancy']\n",
    "    predictions_denorm = predictions * train_std['occupancy'] + train_mean['occupancy']\n",
    "    actuals_denorm = actuals * train_std['occupancy'] + train_mean['occupancy']\n",
    "    \n",
    "    return predictions_denorm, actuals_denorm\n",
    "\n",
    "# Get predictions\n",
    "preds_multi, actuals_multi = predict_on_test_set(multi_lstm_model, multi_window, \n",
    "                                                  test_df, train_mean, train_std)\n",
    "\n",
    "print(f\"Predictions shape: {preds_multi.shape}\")\n",
    "print(f\"Actuals shape: {actuals_multi.shape}\")\n",
    "print(f\"\\nDenormalized predictions range: [{preds_multi.min():.2f}, {preds_multi.max():.2f}]\")\n",
    "print(f\"Denormalized actuals range: [{actuals_multi.min():.2f}, {actuals_multi.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd2afcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions vs actuals for first few samples\n",
    "fig, axes = plt.subplots(3, 1, figsize=(16, 12))\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    # Plot actual values\n",
    "    ax.plot(range(24), actuals_multi[i, :, 0], \n",
    "           label='Actual', linewidth=2, marker='o', alpha=0.7, color='steelblue')\n",
    "    # Plot predictions\n",
    "    ax.plot(range(24), preds_multi[i, :, 0], \n",
    "           label='Predicted', linewidth=2, marker='s', alpha=0.7, color='coral', linestyle='--')\n",
    "    \n",
    "    ax.set_xlabel('Hours Ahead')\n",
    "    ax.set_ylabel('Occupancy')\n",
    "    ax.set_title(f'Sample {i+1}: 24-Hour Multi-Step Forecast')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2500aa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate metrics by forecast horizon\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "horizons = range(1, 25)\n",
    "mae_by_horizon = []\n",
    "rmse_by_horizon = []\n",
    "\n",
    "for h in horizons:\n",
    "    h_idx = h - 1\n",
    "    mae = mean_absolute_error(actuals_multi[:, h_idx, 0], preds_multi[:, h_idx, 0])\n",
    "    rmse = np.sqrt(mean_squared_error(actuals_multi[:, h_idx, 0], preds_multi[:, h_idx, 0]))\n",
    "    mae_by_horizon.append(mae)\n",
    "    rmse_by_horizon.append(rmse)\n",
    "\n",
    "# Plot error by horizon\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "axes[0].plot(horizons, mae_by_horizon, marker='o', linewidth=2, color='steelblue')\n",
    "axes[0].set_xlabel('Forecast Horizon (hours)')\n",
    "axes[0].set_ylabel('MAE')\n",
    "axes[0].set_title('Mean Absolute Error by Forecast Horizon')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(horizons, rmse_by_horizon, marker='s', linewidth=2, color='coral')\n",
    "axes[1].set_xlabel('Forecast Horizon (hours)')\n",
    "axes[1].set_ylabel('RMSE')\n",
    "axes[1].set_title('Root Mean Squared Error by Forecast Horizon')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Forecast Performance by Horizon:\")\n",
    "print(f\"  1-hour ahead:  MAE={mae_by_horizon[0]:.4f}, RMSE={rmse_by_horizon[0]:.4f}\")\n",
    "print(f\"  6-hour ahead:  MAE={mae_by_horizon[5]:.4f}, RMSE={rmse_by_horizon[5]:.4f}\")\n",
    "print(f\"  12-hour ahead: MAE={mae_by_horizon[11]:.4f}, RMSE={rmse_by_horizon[11]:.4f}\")\n",
    "print(f\"  24-hour ahead: MAE={mae_by_horizon[23]:.4f}, RMSE={rmse_by_horizon[23]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36803635",
   "metadata": {},
   "source": [
    "## 12. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e44f579",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"MULTI-STEP FORECASTING - FINAL SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  Station: 1068 (271 charging piles)\")\n",
    "print(f\"  Total samples: {len(df)}\")\n",
    "print(f\"  Features: {len(feature_columns)}\")\n",
    "print(f\"  Train/Val/Test split: 70%/20%/10%\")\n",
    "\n",
    "print(f\"\\nModels Evaluated:\")\n",
    "for name in performance:\n",
    "    print(f\"  {name:15s} - Test MSE: {performance[name][0]:.4f}, Test MAE: {performance[name][1]:.4f}\")\n",
    "\n",
    "print(f\"\\nBest Model: {min(performance, key=lambda x: performance[x][1])}\")\n",
    "print(f\"  (Based on lowest MAE)\")\n",
    "\n",
    "print(f\"\\nMulti-step LSTM Configuration:\")\n",
    "print(f\"  Input window: 24 hours\")\n",
    "print(f\"  Output window: 24 hours\")\n",
    "print(f\"  Architecture: 2 LSTM layers (64, 32 units) + Dense output\")\n",
    "print(f\"  Dropout: 0.2\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"The multi-step LSTM model predicts 24 hours ahead using 24 hours of\")\n",
    "print(\"historical data, including occupancy, prices, weather, and temporal features.\")\n",
    "print(\"Error increases with forecast horizon, as expected for time series forecasting.\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
